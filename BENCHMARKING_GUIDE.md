# ðŸ“Š GuÃ­a de Benchmarking - OMPar

Esta guÃ­a explica cÃ³mo medir y comparar el rendimiento de OMPar antes y despuÃ©s de realizar optimizaciones.

---

## ðŸŽ¯ Objetivo

Medir de forma precisa:
- â±ï¸ **Tiempos de ejecuciÃ³n** de cada componente
- ðŸ’¾ **Uso de memoria** durante la inferencia
- ðŸš€ **Throughput** (inferencias por segundo)
- ðŸ“Š **Comparaciones** antes/despuÃ©s de optimizaciones

---

## ðŸ“‹ Requisitos

Instalar dependencias adicionales para benchmarking:

```bash
pip install psutil gputil
```

---

## ðŸš€ Uso BÃ¡sico

### 1ï¸âƒ£ Crear Baseline (ANTES de optimizaciones)

**IMPORTANTE**: Ejecuta esto ANTES de hacer cualquier cambio al cÃ³digo.

```bash
python benchmark_performance.py \
    --model_weights model \
    --iterations 20 \
    --save-baseline
```

Esto crearÃ¡ el archivo `benchmark_baseline.json` con las mÃ©tricas actuales.

**Salida esperada:**
```
ðŸš€ BENCHMARK DE RENDIMIENTO - OMPar
================================================================================
Dispositivo: cuda
Iteraciones: 20
================================================================================

â±ï¸  Midiendo tiempo de inicializaciÃ³n del modelo...
âœ… InicializaciÃ³n: 2847.32 ms

ðŸ“Š BENCHMARKS INDIVIDUALES
================================================================================

ðŸ“Š Benchmarking: Simple loop
   Iteraciones: 20
   ðŸ”¥ Warm-up... âœ“
   â±ï¸  Ejecutando iteraciones... 5 10 15 20 âœ“
   âœ… Media: 45.23 ms
   ðŸ“ˆ Min/Max: 42.11 / 51.34 ms
   ðŸ“Š Desv. Est.: 2.45 ms
   ðŸš€ Throughput: 22.11 iter/s

...
```

### 2ï¸âƒ£ Realizar Optimizaciones

Implementa tus mejoras en C++/CUDA siguiendo las recomendaciones.

### 3ï¸âƒ£ Comparar con Baseline (DESPUÃ‰S de optimizaciones)

```bash
python benchmark_performance.py \
    --model_weights model \
    --iterations 20 \
    --baseline benchmark_baseline.json \
    --output benchmark_optimized.json
```

**Salida esperada:**
```
ðŸ“Š COMPARACIÃ“N CON BASELINE
================================================================================

Simple loop:
  Baseline:  45.23 ms
  Actual:    12.34 ms
  Speedup:   3.67x
  Mejora:    +72.7%

Reduction:
  Baseline:  48.91 ms
  Actual:    13.21 ms
  Speedup:   3.70x
  Mejora:    +73.0%

...
```

---

## ðŸ“Š MÃ©tricas Medidas

### Tiempos de EjecuciÃ³n

| MÃ©trica | DescripciÃ³n |
|---------|-------------|
| **mean_ms** | Tiempo promedio de ejecuciÃ³n |
| **median_ms** | Mediana (mÃ¡s robusta a outliers) |
| **min_ms** | Mejor caso |
| **max_ms** | Peor caso |
| **stdev_ms** | DesviaciÃ³n estÃ¡ndar (variabilidad) |
| **p95_ms** | Percentil 95 (95% de casos son mÃ¡s rÃ¡pidos) |
| **p99_ms** | Percentil 99 (99% de casos son mÃ¡s rÃ¡pidos) |

### Componentes del Pipeline

El benchmark mide cada etapa por separado:

1. **classification_ms**: Tiempo de OMPify (detectar si es paralelizable)
2. **generation_ms**: Tiempo de MonoCoder (generar pragma)
3. **formatting_ms**: Tiempo de formateo del pragma
4. **total_ms**: Tiempo total end-to-end

### Memoria

- **memory_before_mb**: Memoria antes de inferencia
- **memory_after_mb**: Memoria despuÃ©s de inferencia
- **memory_peak_mb**: Pico de memoria durante inferencia
- **memory_increase_mb**: Incremento de memoria

### Throughput

- **iterations_per_second**: CuÃ¡ntas inferencias por segundo
- **ms_per_iteration**: Milisegundos por inferencia

---

## ðŸ”¬ Casos de Prueba

El benchmark incluye 5 casos de prueba representativos:

1. **Simple loop**: InicializaciÃ³n bÃ¡sica
2. **Reduction**: Suma acumulativa
3. **Array copy**: Copia de arrays
4. **Element-wise operation**: OperaciÃ³n elemento a elemento
5. **Complex operation**: Bucle anidado con operaciones complejas

---

## ðŸ“ˆ InterpretaciÃ³n de Resultados

### Speedup

```
Speedup = Tiempo_Baseline / Tiempo_Actual
```

- **Speedup > 1.0**: Mejora (mÃ¡s rÃ¡pido)
- **Speedup = 1.0**: Sin cambios
- **Speedup < 1.0**: RegresiÃ³n (mÃ¡s lento)

### Ejemplos

| Speedup | InterpretaciÃ³n |
|---------|----------------|
| 2.0x | 2 veces mÃ¡s rÃ¡pido (50% del tiempo original) |
| 3.0x | 3 veces mÃ¡s rÃ¡pido (33% del tiempo original) |
| 10.0x | 10 veces mÃ¡s rÃ¡pido (10% del tiempo original) |

### Mejora Porcentual

```
Mejora% = ((Tiempo_Baseline - Tiempo_Actual) / Tiempo_Baseline) Ã— 100
```

- **+50%**: ReducciÃ³n del 50% en tiempo (2x mÃ¡s rÃ¡pido)
- **+75%**: ReducciÃ³n del 75% en tiempo (4x mÃ¡s rÃ¡pido)
- **+90%**: ReducciÃ³n del 90% en tiempo (10x mÃ¡s rÃ¡pido)

---

## ðŸŽ¯ Objetivos de OptimizaciÃ³n

### Fase 1: Quick Wins
- **Objetivo**: 3-5x speedup
- **Tiempo**: 1-2 semanas
- **Implementaciones**: DFG Extractor C++, Cache, ONNX Runtime

### Fase 2: Optimizaciones Medias
- **Objetivo**: 8-12x speedup
- **Tiempo**: 2-4 semanas
- **Implementaciones**: Static Analyzer, Pipeline Paralelo, TensorRT

### Fase 3: ProducciÃ³n
- **Objetivo**: 10-15x speedup
- **Tiempo**: 4-6 semanas
- **Implementaciones**: CLI standalone, Memory optimizations

---

## ðŸ“ Ejemplo Completo

### Paso 1: Baseline

```bash
# Crear baseline ANTES de optimizaciones
python benchmark_performance.py --save-baseline --iterations 50

# Resultado: benchmark_baseline.json creado
```

### Paso 2: Implementar OptimizaciÃ³n

Por ejemplo, implementar DFG Extractor en C++:

```bash
# Compilar mÃ³dulo C++
cd cpp_extensions
mkdir build && cd build
cmake ..
make -j8
cd ../..
```

### Paso 3: Comparar

```bash
# Ejecutar benchmark con optimizaciÃ³n
python benchmark_performance.py \
    --baseline benchmark_baseline.json \
    --iterations 50 \
    --output benchmark_with_cpp_dfg.json

# Ver comparaciÃ³n automÃ¡tica
```

### Paso 4: Analizar Resultados

```bash
# Ver archivo JSON con resultados detallados
cat benchmark_with_cpp_dfg.json | jq '.benchmarks[0]'
```

---

## ðŸ” AnÃ¡lisis Avanzado

### Comparar MÃºltiples Versiones

```python
import json
import pandas as pd

# Cargar resultados
baseline = json.load(open('benchmark_baseline.json'))
opt1 = json.load(open('benchmark_opt1.json'))
opt2 = json.load(open('benchmark_opt2.json'))

# Crear tabla comparativa
data = []
for b, o1, o2 in zip(baseline['benchmarks'], opt1['benchmarks'], opt2['benchmarks']):
    data.append({
        'Test': b['name'],
        'Baseline (ms)': b['times']['mean_ms'],
        'Opt1 (ms)': o1['times']['mean_ms'],
        'Opt2 (ms)': o2['times']['mean_ms'],
        'Speedup Opt1': b['times']['mean_ms'] / o1['times']['mean_ms'],
        'Speedup Opt2': b['times']['mean_ms'] / o2['times']['mean_ms']
    })

df = pd.DataFrame(data)
print(df.to_markdown(index=False))
```

### Visualizar Resultados

```python
import matplotlib.pyplot as plt

# GrÃ¡fico de speedup
tests = [b['name'] for b in baseline['benchmarks']]
speedups = [
    baseline['benchmarks'][i]['times']['mean_ms'] / 
    opt1['benchmarks'][i]['times']['mean_ms']
    for i in range(len(tests))
]

plt.figure(figsize=(10, 6))
plt.bar(tests, speedups)
plt.axhline(y=1.0, color='r', linestyle='--', label='Baseline')
plt.ylabel('Speedup (x)')
plt.title('Speedup por OptimizaciÃ³n')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('speedup_comparison.png')
```

---

## âš ï¸ Consideraciones Importantes

### 1. Warm-up

El benchmark hace un "warm-up" antes de medir:
- Carga modelos en memoria
- Inicializa CUDA kernels
- Llena caches

**No omitas el warm-up** o tendrÃ¡s mediciones incorrectas.

### 2. NÃºmero de Iteraciones

- **MÃ­nimo recomendado**: 10 iteraciones
- **Recomendado**: 20-50 iteraciones
- **Para paper/publicaciÃ³n**: 100+ iteraciones

MÃ¡s iteraciones = resultados mÃ¡s confiables pero mÃ¡s tiempo.

### 3. Variabilidad

Si `stdev_ms` es muy alto (>10% de `mean_ms`):
- Aumentar nÃºmero de iteraciones
- Cerrar otros programas
- Verificar throttling de GPU/CPU

### 4. GPU vs CPU

Los resultados varÃ­an significativamente:
- **GPU**: Mejor para batches grandes
- **CPU**: Mejor para latencia baja

Siempre especifica quÃ© dispositivo usaste.

---

## ðŸ“Š Formato de Resultados JSON

```json
{
  "timestamp": "2026-01-04T20:59:00",
  "device": "cuda",
  "initialization_time_ms": 2847.32,
  "system_info": {
    "cpu": {...},
    "memory": {...},
    "gpu": {...}
  },
  "benchmarks": [
    {
      "name": "Simple loop",
      "iterations": 20,
      "code_length": 45,
      "times": {
        "mean_ms": 45.23,
        "median_ms": 44.87,
        "min_ms": 42.11,
        "max_ms": 51.34,
        "stdev_ms": 2.45,
        "p95_ms": 49.12,
        "p99_ms": 50.87
      },
      "components": {
        "classification_ms": {...},
        "generation_ms": {...},
        "formatting_ms": {...},
        "total_ms": {...}
      },
      "throughput": {
        "iterations_per_second": 22.11,
        "ms_per_iteration": 45.23
      }
    }
  ],
  "batch_benchmark": {...},
  "memory_benchmark": {...}
}
```

---

## ðŸŽ“ Tips y Mejores PrÃ¡cticas

1. **Siempre crea baseline ANTES** de hacer cambios
2. **Usa mismo hardware** para comparaciones justas
3. **Cierra otros programas** durante benchmarking
4. **Ejecuta mÃºltiples veces** y promedia
5. **Documenta cambios** entre versiones
6. **Guarda todos los JSONs** para referencia futura
7. **Verifica que optimizaciones no rompan correctitud**

---

## ðŸ”— Archivos Relacionados

- Script de benchmark: [`benchmark_performance.py`](benchmark_performance.py)
- Pruebas de correctitud: [`simple_tests.py`](simple_tests.py)
- Resultados de pruebas: [`SIMPLE_TESTS_RESULTS.md`](SIMPLE_TESTS_RESULTS.md)

---

## ðŸ“ž Troubleshooting

### Error: "No module named 'psutil'"

```bash
pip install psutil gputil
```

### Error: "CUDA out of memory"

Reduce el nÃºmero de iteraciones o usa CPU:

```bash
CUDA_VISIBLE_DEVICES="" python benchmark_performance.py ...
```

### Resultados inconsistentes

1. Cerrar otros programas
2. Aumentar iteraciones
3. Verificar temperatura de GPU/CPU
4. Deshabilitar turbo boost si es necesario

---

## âœ… Checklist Pre-OptimizaciÃ³n

- [ ] Ejecutar `benchmark_performance.py --save-baseline`
- [ ] Verificar que `benchmark_baseline.json` existe
- [ ] Documentar versiÃ³n actual del cÃ³digo
- [ ] Hacer commit en git antes de cambios
- [ ] Ejecutar `simple_tests.py` para verificar correctitud

## âœ… Checklist Post-OptimizaciÃ³n

- [ ] Ejecutar `benchmark_performance.py --baseline benchmark_baseline.json`
- [ ] Verificar speedup obtenido
- [ ] Ejecutar `simple_tests.py` para verificar correctitud
- [ ] Documentar cambios realizados
- [ ] Guardar resultados con nombre descriptivo
- [ ] Hacer commit con resultados

---

**Â¡Listo para empezar a optimizar! ðŸš€**
