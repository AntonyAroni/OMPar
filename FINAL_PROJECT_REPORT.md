# OMPar: Informe T√©cnico Final del Proyecto

**Estado del Proyecto**: Finalizado / Optimizado üöÄ  
**Versi√≥n**: 2.0 (High-Performance Edition)  
**Fecha**: Enero 2026

---

## 1. üìñ Introducci√≥n y Visi√≥n General

**OMPar** (OpenMP Parallelization) es un sistema avanzado de inteligencia artificial dise√±ado para resolver uno de los desaf√≠os m√°s antiguos en la computaci√≥n de alto rendimiento: la **paralelizaci√≥n autom√°tica de c√≥digo legado**.

El objetivo principal es tomar c√≥digo fuente serial escrito en C/C++ y, mediante t√©cnicas de aprendizaje profundo (Deep Learning), identificar bucles que pueden ejecutarse en paralelo y generar autom√°ticamente las directivas OpenMP (`#pragma omp ...`) correctas, preservando la sem√°ntica y correcci√≥n del programa.

Este informe documenta la transformaci√≥n del proyecto desde un prototipo acad√©mico en Python hasta una **herramienta de producci√≥n de alto rendimiento**, lograda a trav√©s de una reingenier√≠a profunda y optimizaci√≥n por hardware.

---
## 2. üèóÔ∏è Arquitectura del Sistema

El sistema OMPar opera mediante un **pipeline secuencial de tres etapas cr√≠ticas**, dise√±ado para minimizar la latencia total y maximizar el throughput. Cada etapa transforma progresivamente el c√≥digo fuente hasta producir una versi√≥n paralelizada y sem√°nticamente equivalente.

---

### 2.1 Entrada del Sistema

- **Entrada**: C√≥digo fuente serial escrito en **C/C++**.
- El c√≥digo es analizado funci√≥n por funci√≥n, con especial √©nfasis en bucles (`for`, `while`) candidatos a paralelizaci√≥n.

---

### 2.2 Etapa 1: An√°lisis Est√°tico (CPU)

Esta etapa se ejecuta completamente en **CPU** y tiene como objetivo comprender la estructura y las dependencias del c√≥digo.

**Componentes:**

- **Parser**
  - Construye el **√Årbol de Sintaxis Abstracta (AST)** del c√≥digo fuente.
- **DFG Extractor (Data Flow Graph)**
  - Analiza las dependencias de datos (lectura y escritura de variables).
  - Detecta posibles *data races* y dependencias que impedir√≠an la ejecuci√≥n paralela.

**Salida:**

- Una **representaci√≥n intermedia** que combina:
  - Informaci√≥n sint√°ctica (AST).
  - Informaci√≥n sem√°ntica y de dependencias (Data Flow).

---

### 2.3 Etapa 2: Inteligencia Artificial (GPU)

Esta etapa se ejecuta en **GPU** y constituye el n√∫cleo de decisi√≥n y generaci√≥n del sistema.

#### 2.3.1 OMPify: Clasificador

- Modelo discriminativo basado en **GraphCodeBERT**.
- Eval√∫a la representaci√≥n intermedia y determina:
  - Si un bucle es **paralelizable** o **no paralelizable**.
  - El tipo de paralelizaci√≥n requerida (privatizaci√≥n de variables, reducciones, etc.).

**Resultados posibles:**

- **No paralelizable**  
  ‚Üí El pipeline termina y el c√≥digo se devuelve sin modificaciones.
- **Paralelizable**  
  ‚Üí El flujo contin√∫a hacia el generador.

#### 2.3.2 MonoCoder: Generador

- Modelo generativo basado en **GPT-NeoX (160M)**.
- Produce el texto exacto de las directivas **OpenMP (`#pragma omp ...`)** necesarias.
- La generaci√≥n se realiza token a token, optimizada para baja latencia.

**Salida:**

- Secuencia de tokens que representan las directivas OpenMP.

---

### 2.4 Etapa 3: Post-Procesamiento

Esta etapa convierte la salida del modelo generativo en c√≥digo fuente final listo para su uso.

**Componentes:**

- **Decodificador**
  - Convierte los tokens generados en texto legible.
- **Formateador**
  - Inserta las directivas OpenMP en las posiciones correctas del c√≥digo.
  - Respeta el estilo original del c√≥digo fuente.

**Salida Final:**

- **C√≥digo C/C++ paralelizado**, con directivas `#pragma omp` correctamente integradas.

---

### 2.5 Flujo Resumido del Pipeline

1. Entrada de c√≥digo fuente C/C++.
2. An√°lisis est√°tico y extracci√≥n de dependencias (CPU).
3. Clasificaci√≥n de paralelizaci√≥n y generaci√≥n de directivas (GPU).
4. Decodificaci√≥n, formateo y salida de c√≥digo paralelizado.


## 3. üõ†Ô∏è Tecnolog√≠as e Infraestructura

La robustez del proyecto se basa en un stack tecnol√≥gico h√≠brido seleccionado para equilibrar flexibilidad y velocidad m√°xima.

| Categor√≠a | Tecnolog√≠as | Prop√≥sito |
|-----------|-------------|-----------|
| **Lenguajes** | Python 3.12, C++17 | Python para orquestaci√≥n/IA, C++ para c√≥mputo intensivo (Parsing). |
| **Deep Learning** | PyTorch 2.5 (CUDA 12.1) | Framework base para entrenamiento y manipulaci√≥n de tensores. |
| **Inferencia** | **NVIDIA TensorRT 10.x** | Motor de compilaci√≥n de redes neuronales para m√°xima velocidad en GPU. |
| **Interoperabilidad** | **pybind11** | Puente de alta eficiencia para llamar c√≥digo C++ desde Python sin copia de memoria. |
| **Parsing** | Tree-sitter | Generador de parsers incremental y ultra-r√°pido. |
| **Formato** | ONNX, Safetensors | Est√°ndares de intercambio de modelos seguros y portables. |

---

## 4. ÔøΩ Fases de Desarrollo y Optimizaci√≥n

La transformaci√≥n de OMPar se llev√≥ a cabo en tres fases estrat√©gicas, abordando los cuellos de botella m√°s severos en orden de impacto.

### üü° Fase 1: Optimizaci√≥n del Pre-procesamiento (Data Bottleneck)
*   **Problema**: El extractor original en Python era extremadamente lento (0.1ms por bucle simple, exponencial en bucles complejos) debido al overhead del int√©rprete y gesti√≥n de objetos.
*   **Soluci√≥n**: Se dise√±√≥ una **extensi√≥n nativa en C++** que interact√∫a directamente con la librer√≠a `tree-sitter`. Se crearon bindings Python con `pybind11` para una integraci√≥n transparente.
*   **Resultado**: Aceleraci√≥n masiva de **4.12x** en el tiempo de parsing. De 82¬µs a 19¬µs por item.

### üü¢ Fase 2: Optimizaci√≥n de Memoria e Inferencia (Compute Bottleneck)
*   **Problema**: La inferencia en precisi√≥n est√°ndar (FP32) saturaba la memoria VRAM (casi 4GB) y limitaba el tama√±o de batch en GPUs de consumo.
*   **Soluci√≥n**: Migraci√≥n completa del pipeline a **Half Precision (FP16)**. Esto no solo reduce el uso de memoria a la mitad, sino que activa los Tensor Cores de las GPUs modernas NVIDIA.
*   **Resultado**: Reducci√≥n del uso de VRAM en un **50%** (1.8GB vs 3.6GB) y una mejora de velocidad de **1.8x**.

### üîµ Fase 3: Aceleraci√≥n por Hardware y Latencia (Latency Bottleneck)
*   **Problema**: PyTorch (Eager Mode) tiene un overhead de lanzamiento de kernels y gesti√≥n de memoria din√°mica que a√±ade latencia, especialmente en procesamiento de tokens secuenciales (generaci√≥n de texto).
*   **Soluci√≥n**: Implementaci√≥n de **NVIDIA TensorRT**. 
    *   Se convirti√≥ el modelo a ONNX y luego se compil√≥ un **Engine espec√≠fico para la GPU**.
    *   Se utiliz√≥ **Static Shape** (Dimensiones fijas) para permitir que el compilador fusionara operaciones (kernel fusion) y pre-calculara grafos de memoria.
    *   **Arquitectura H√≠brida**: Se opt√≥ por un wrapper Python robusto sobre la implementaci√≥n C++ inestable, garantizando la misma velocidad de ejecuci√≥n en GPU (el trabajo pesado lo hace el Engine) pero con total estabilidad de sistema.
*   **Resultado Final**: Latencia ultra-baja y constante (~250ms por inferencia completa), maximizando el throughput del sistema.

---

## 5. üèÜ Resultados Finales de Rendimiento

El impacto acumulado de las tres fases transform√≥ la usabilidad de la herramienta.

### Comparativa Global

| M√©trica | Versi√≥n Inicial (Baseline) | Versi√≥n Final (Optimizada) | Factor de Mejora |
|:--------|:--------------------------:|:--------------------------:|:----------------:|
| **Tiempo Total (Parsing + Inferencia)** | ~485 ms | **~260 ms** | **1.87x M√°s R√°pido** |
| **Throughput (Items/seg)** | 2.06 | **3.85** | **+87% Capacidad** |
| **Consumo de Memoria VRAM** | 3.6 GB | **1.8 GB** | **-50% Recursos** |
| **Parsing de C√≥digo (C++)** | 0.08 ms | **0.02 ms** | **4.12x M√°s R√°pido** |

### An√°lisis de Viabilidad
Con estos tiempos, OMPar ahora es viable para integrarse en **pipelines de CI/CD** o incluso como plugin de IDEs en tiempo real, donde la latencia de respuesta es cr√≠tica.

---

## 6. Conclusiones

El proyecto OMPar ha demostrado exitosamente que las t√©cnicas modernas de compilaci√≥n de IA (TensorRT, ONNX) combinadas con programaci√≥n de sistemas eficiente (C++) pueden desbloquear niveles de rendimiento inaccesibles para implementaciones puras en Python.

El sistema final es:
1.  **R√°pido**: Capaz de procesar repositorios en segundos.
2.  **Ligero**: Ejecutable en GPUs de gama media (incluso port√°tiles con 4GB VRAM).
3.  **Robusto**: Probado y validado con suites de benchmarks y manejo de errores nativo.

Este trabajo sienta las bases para futuras expansiones, como el soporte para OpenACC (GPUs) o MPI (Clusters), utilizando el mismo motor de inferencia optimizado.
